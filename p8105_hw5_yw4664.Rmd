---
title: "p8105_hw5_yw4664"
author: "Yijun Wang"
date: "2025-11-01"
output: github_document
---

## Problem 1

### Loading Libraries

```{r, echo=TRUE, results='hide', message=FALSE}

library(tidyverse)

library(ggplot2)

set.seed(1) 

source("birthday_functions.R")
```

### Run this function 10000 times for each group size between 2 and 50, compute probability

```{r}
n_range <- 2:50
B <- 10000

sim_results <- tibble(n = n_range) |>
  mutate(
    prob_dup = map_dbl(n, ~ mean(replicate(B, bday_collision(.x))))
  )

```

### Make a plot showing the probability as a function of group size

```{r}

sim_results |>
  ggplot(aes(x = n, y = prob_dup)) +
  geom_line(size = 1) +
  geom_point(size = 1.2) +
  geom_hline(yintercept = 0.5, linetype = 2) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Probability of â‰¥2 people sharing a birthday, 10,000 sims",
    x = "Group size",
    y = "Estimated probability"
  ) +
  theme_minimal(base_size = 14)

```

The curve starts very close to 0 for small groups, meaning birthday collisions are very unlikely.

The probability rises sharply between roughly 15 and 30 people, crossing the 50 % line around n â‰ˆ 23, which is the famous birthday paradox point.

Beyond about 45 people, the probability approaches 1, implying that in any moderately large group, shared birthdays are almost certain.

## Problem 2

### Loading Libraries

```{r, echo=TRUE, results='hide', message=FALSE}

library(tidyverse)

library(ggplot2)

library(broom)

set.seed(1)

source("power_functions.R")

```

### Simulate 5000 datasets for mu = 0,1,2,3,4,5,6

```{r}
mu_grid <- 0:6
B <- 5000

sim_p2 <- map_dfr(mu_grid, ~ bind_rows(replicate(B, power_one_run(.x), simplify = FALSE)))

```

### Plot showing the proportion of times the null was rejected on the y axis and the true value of ðœ‡ on the x axis

```{r}

sim_p2 %>%
  group_by(mu_true) |>
  summarise(power = mean(reject), .groups = "drop") |>
  ggplot(aes(mu_true, power)) +
  geom_line(size = 1) + geom_point(size = 1.3) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Power vs True Î¼ (n=30, Ïƒ=5, Î±=0.05)", x = "True Î¼", y = "Power") +
  theme_minimal(base_size = 13)


```

### Plot showing the average estimate of ðœ‡Ì‚ on the y axis and the true value of ðœ‡ on the x axis and the average estimate of ðœ‡Ì‚ only in samples for which the null was rejected on the y axis and the true value of ðœ‡ on the x axis.

```{r}
sim_p2 |>
  group_by(mu_true) |>
  summarise(
    muhat_overall = mean(mu_hat),
    muhat_condrej = mean(mu_hat[reject]),
    .groups = "drop"
  ) |>
  pivot_longer(-mu_true, names_to = "type", values_to = "avg_muhat") |>
  mutate(type = recode(type,
                       muhat_overall = "E[Î¼Ì‚]",
                       muhat_condrej = "E[Î¼Ì‚|reject]")) |>
  ggplot(aes(mu_true, avg_muhat, color = type)) +
  geom_line(size = 1) + geom_point(size = 1.3) +
  labs(title = "Average Î¼Ì‚ (all vs rejection)",
       x = "True Î¼", y = "Average Î¼Ì‚", color = NULL) +
  theme_minimal(base_size = 13)

```

No, the sample average of Î¼Ì‚ across tests for which the null was rejected is not equal to the true Î¼, when Î¼ is small.

This happens because only samples with unusually large Î¼Ì‚ are likely to be significant when the true effect is small.

## Problem 3

### Loading Libraries

```{r, echo=TRUE, results='hide', message=FALSE}

library(tidyverse)

library(ggplot2)

set.seed(1) 

h <- read_csv("data/homicide-data.csv")
```

### Describe the raw data

```{r}

dim(h)  

names(h) 

h |> summarise(
  n_cities  = n_distinct(city),
  n_states  = n_distinct(state),
  years_min = min(lubridate::year(as.Date(reported_date)), na.rm = TRUE),
  years_max = max(lubridate::year(as.Date(reported_date)), na.rm = TRUE)
)

h |> slice_head(n = 5)

```

The data includes 52,179 rows and 12 variables.

Variables include uid, reported_date, victim_last, victim_first, victim_race, victim_age, victim_sex, city, state, lat, lon, and disposition.

Each row is one homicide case in one of 50 cities across 28 states; it includes victim demographics, location, and the case disposition.

### Create city_state and summarize totals and unsolved counts by city

```{r}

h2 <- h |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  )

by_city <- h2 |>
  group_by(city_state) |>
  summarise(
    n_total    = n(),
    n_unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

by_city |> slice_head(n = 10)

```

###  prop.test function for the city of Baltimore, MD to estimate the proportion of homicides that are unsolved

```{r}

balt <- h2 %>% filter(city_state == "Baltimore, MD")

balt_test <- prop.test(
  x = sum(balt$unsolved, na.rm = TRUE),
  n = nrow(balt)
)

balt_tidy <- tidy(balt_test) %>%
  select(estimate, conf.low, conf.high)

balt_tidy

```

### prop.test for every city via a tidy pipeline

```{r}

city_prop <- function(df_city) {
  tidy(prop.test(
    x = sum(df_city$unsolved, na.rm = TRUE),
    n = nrow(df_city)
  )) |> select(estimate, conf.low, conf.high)
}

city_est <- h2 |>
  nest(data = -city_state) |>      
  mutate(tidy_res = map(data, city_prop)) |>
  select(-data) |>
  unnest(tidy_res) |>
  arrange(estimate) |>
  mutate(city_state = factor(city_state, levels = city_state))

city_est

```

### Plot estimates and CIs for each city

```{r city-plot, fig.width=8, fig.height=15, dpi=150, out.width='100%'}

ggplot(city_est, aes(x = estimate, y = city_state)) +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0) +
  geom_point(size = 2) +
  labs(
    title = "Proportion of unsolved homicides (95% CI) by city",
    x = "Estimated proportion unsolved", y = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(axis.text.y = element_text(size = 9))

```